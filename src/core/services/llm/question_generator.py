"""
Question Generator Service.

Generates realistic questions about a brand using LLM.
Simulates a real user searching for information about the brand.
"""

import logging

from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt, wait_exponential

from src.core.config import DEFAULT_NUM_QUESTIONS, DEFAULT_QUESTION_LLM, QUESTION_LLM_TEMPERATURE
from src.core.services.llm.llm_factory import create_llm

logger = logging.getLogger(__name__)


class QuestionsResponse(BaseModel):
    """
    Structure for validating questions generated by the LLM.

    Ensures we always get a list of strings in the correct format.
    """

    questions: list[str] = Field(
        description="List of realistic questions about the brand that a typical user would ask",
        min_length=1,
        max_length=10,
    )


@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def generate_questions(
    brand: str,
    num_questions: int = DEFAULT_NUM_QUESTIONS,
    question_llm: str = DEFAULT_QUESTION_LLM,
    brand_context: str | None = None,
) -> list[str]:
    """
    Generate realistic questions about a brand using LLM.

    Simulates a real user searching for information about the brand.
    Uses structured output to guarantee the format.

    Args:
        brand: Name of the brand to generate questions about (e.g., "Nike", "Brevo")
        num_questions: Number of questions to generate (default: 2, will be used as guidance)
        question_llm: LLM specification in format "provider:model" (default: "openai:gpt-4.1-mini")
        brand_context: Optional factual context about the brand (generated by node 0)

    Returns:
        List of questions (List[str])

    Raises:
        ValueError: If provider is not supported or API key is missing
        Exception: If LLM call fails after retries
    """
    try:
        llm = create_llm(question_llm, temperature=QUESTION_LLM_TEMPERATURE)

        structured_llm = llm.with_structured_output(QuestionsResponse)

        brand_context_section = ""
        if brand_context:
            brand_context_section = f"""
CONTEXT ABOUT THE BRAND:
{brand_context}

Use this context to understand what {brand} does, especially if it's a less-known brand.
"""

        prompt = f"""You are a real user searching for information about {brand}.

{brand_context_section}

Generate {num_questions} realistic questions that a typical user would ask when researching this brand.
These questions should be:
- Natural and conversational (like real Google/Perplexity searches)
- Diverse (products, competitors, recommendations, reviews, pricing, availability)
- Specific to the brand and its industry
- Questions that users would actually type into search engines or AI assistants
- Include comparisons with competitors from the same industry

IMPORTANT: One and only one of the questions MUST be a comprehensive, exhaustive question about the brand's negative aspects, weaknesses, complaints, or criticisms. This question should be formulated to get a detailed, in-depth analysis of what users, customers, or experts say are the main problems, drawbacks, or negative points about {brand}. Make it natural and realistic, like a real user would ask when doing thorough research.
If the brand is well-known, you can use your general knowledge in addition to the context provided above.
The context above is provided to help you understand the brand, especially for less-known brands. Use it to avoid hallucinations, but don't limit yourself to it if you have broader knowledge about well-known brands.

Brand: {brand}

Generate questions that sound like real user queries, not marketing questions.
Generate only the questions, ensuring they are realistic and varied."""

        response = structured_llm.invoke(prompt)

        questions = response.questions

        logger.info(f"Generated {len(questions)} questions for brand: {brand}")
        return questions

    except Exception as e:
        logger.error(f"Failed to generate questions for brand '{brand}': {str(e)}")
        raise
